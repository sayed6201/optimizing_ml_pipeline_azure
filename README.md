# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains data about marketing campaigns based on phone calls of a Portuguese banking institution. We seek to predict to predict if the client will subscribe a term deposit.

The best perfoming model was Voting Ensemble model having accuracy 91.56 % followed by MaxAbsScaler, LightGBM having accuracy of 91.39%. The best performing model was derived using Azure AutoML.
The same dataset was also used to train Logistic regression model and Parameters were tuned using Azure Hyperdrive which resulted in 91.32% accuracy.

## Scikit-learn Pipeline

### The pipeline architecture
* The tabular dataset is created using TabularDatasetFactory from the the link
* The dataset is cleaned using python script. cleaning includes encoding categorical values, representing some non numeric values into numeric format 
* The data is splitted into test and train set. the test:train ration is 20:80
* The data is used to train LogisticRegression from sklearn.linear_model. SKLearn estimator from azureml.train.sklearn is used to run the trainnig script
* The model LogisticRegression takes two hyperparameters inverse of regularization strength (--C) and Max iterations (--max_iter), Using Azure Hyper Drive this paremeters are tuned to get model with best accuracy.

Screenshot shows HyperDrive Model accuracy in descending order after Run completion: 
![alt text](https://github.com/sayed6201/optimizing_ml_pipeline_azure/blob/master/screenshots/hyperdrive_child_run_completed.png "hyerdrive run completed")

Screenshot shows HyperDrive best model run : 
![alt text](https://github.com/sayed6201/optimizing_ml_pipeline_azure/blob/master/screenshots/hyperdrive_best_model.png "hyerdrive best model run completed")



### Benefits of choosing Random Parameter Sampler
* Parameter sampling method helps us to choose proper hyperparameter value for our model, in Azure Machine Learning supports the following methods:
    * Random sampling
    * Grid sampling
    * Bayesian sampling
I opted Random sampling method.
* In random sampling, hyperparameter values are randomly selected from the defined search space. It also supports early termination of low-performance runs.
    * The discrete values chosen for inverse of regularization strength were 0.01,5,20,100,500, where the lower value indicates strong regularization, and the best model had inverse of regularization strength of 0.01.
    * The discrete values chosen for Max iteration were 10,50,100,150,200. the best model had Max iteration of 150.


### Benefits of choosing Bandit Policy for early stopping
* The early stopping policies automatically terminate poorly performing runs and improves computational efficiency.
* Bandit policy is based on slack factor and evaluation interval. I have defined slackfactor = 0.1, The policy terminates runs where the primary metric is not within the specified slack factor compared to the best performing run.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
* Automated machine learning or AutoML is the process of automating some of the the iterative tasks and time consuming of machine learning pipeline such as feature engineering, hyperprameter tuning, model training etc.

* To configure AutoML i have specified following parameters in AutoMLConfig class from azureml.train.automl.
   * task: refers to problem The type of task to run, we are solving a classification problem
   * primary_metric: The metric that AutoML will optimize for model selection. I'm specifying 'Accuracy'
   * n_cross_validations: Number of cross validations to be performed, I'm specifying 'Accuracy'
   * training_data: data to be used for training the model.
   * max_concurrent_iterations : Rrefers to the maximum number of iterations executed in parallel, i have spevified 4
   * enable_early_stopping: if set true it will early terminate if the score is not improving. 
   * label_column_name : label of column that will be predicted.
   * max_cores_per_iteration: Refers to The maximum number of threads to be used for training iteration.I'm specyfying -1, which refers to use all the possible cores per iteration per child-run.
   * experiment_timeout_minutes : i specified 30 minutes which means maximum amount of time that all iterations combined can take before the experiment terminates.
   
Screenshot shows AutoML configuration code from notebook: 
![alt text](https://github.com/sayed6201/optimizing_ml_pipeline_azure/blob/master/screenshots/automl_config.PNG "automl configuration")



* Both AutoML and HyperDrive was configured with same dataset and same primary metric. AutoML was able to train model having the best accuracy. the screenshot below shows models with best accuracy in descending order. VotingEnsemble model outperformed all other models, scoring accuracy of 91.58 %.


Screenshot different model trained by AutoML with accuray : 
![alt text](https://github.com/sayed6201/optimizing_ml_pipeline_azure/blob/master/screenshots/all_automl_models.png "automl best models")

Screenshot shows Best AutoML model run detail: 
![alt text](https://github.com/sayed6201/optimizing_ml_pipeline_azure/blob/master/screenshots/best_automl_model.png "automl best model")


* From the trained model explanation we can understand what features are having greater impact on decision making, from the screenshot below we can see that employment variation rate, month, number of employees , duration, pdays etc has higher importance in prediction


Screenshot taken from AutoML model explanation showing some important features: 
![alt text](https://github.com/sayed6201/optimizing_ml_pipeline_azure/blob/master/screenshots/automl_explanation_global_importance.png "automl global importance")


## Pipeline comparison

* The model trained through Azure AutoML outperformed the accuracy of Logistic regression model that was tuned using Azure HyperDrive. In AutoML some of top best models in desceinding order were VotingEnsemble, MaxAbsScaler,LightGBM,StandardScalerWrapper,XGBoostClassifier. VotingEnsemble had the best accuracy score of 91.58 %. The Logistic regression tuned with HyperDrive had the accuracy of 91.32%.

* Though the difference of accuracy for model trained with AutoML and HyperDrive was little, Azure AutoML made it easy to train multiple models in a short time, with hyperdrive it would have required to create different pipeline for different models. 


## Future work

I used cleaned data to train AutoML models, i would like to see if AutoML perform better than the current model when featurization it set to true. besides the dataset is imbalanced so i would like to train AutoML and HyperDrive models using F1-score, AUC and other primary metrics.


## Proof of cluster clean up
The image shows cluster getting deleted
